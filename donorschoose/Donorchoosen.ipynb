{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.4.10\n",
      "  latest version: 4.5.4\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ubuntu/anaconda/envs/kaggle\n",
      "\n",
      "  added / updated specs: \n",
      "    - nltk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    numpy-1.14.3               |   py36hcd700cb_1          41 KB  anaconda\n",
      "    mkl_fft-1.0.1              |   py36h3010b51_0         140 KB  anaconda\n",
      "    ca-certificates-2018.03.07 |                0         124 KB  anaconda\n",
      "    openssl-1.0.2o             |       h20670df_0         3.4 MB  anaconda\n",
      "    nltk-3.3.0                 |           py36_0         2.0 MB  anaconda\n",
      "    certifi-2018.4.16          |           py36_0         142 KB  anaconda\n",
      "    mkl_random-1.0.1           |   py36h629b387_0         373 KB  anaconda\n",
      "    numpy-base-1.14.3          |   py36h9be14a7_1         4.1 MB  anaconda\n",
      "    libgfortran-ng-7.2.0       |       hdf63c60_3         1.2 MB  anaconda\n",
      "    blas-1.0                   |              mkl           6 KB  anaconda\n",
      "    scipy-1.1.0                |   py36hfc37229_0        18.1 MB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        29.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    libgfortran-ng:  7.2.0-hdf63c60_3              anaconda   \n",
      "    mkl_fft:         1.0.1-py36h3010b51_0          anaconda   \n",
      "    mkl_random:      1.0.1-py36h629b387_0          anaconda   \n",
      "    nltk:            3.3.0-py36_0                  anaconda   \n",
      "    numpy-base:      1.14.3-py36h9be14a7_1         anaconda   \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    certifi:         2018.4.16-py36_0              conda-forge --> 2018.4.16-py36_0      anaconda\n",
      "    openssl:         1.0.2o-0                      conda-forge --> 1.0.2o-h20670df_0     anaconda\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "    blas:            1.1-openblas                  conda-forge --> 1.0-mkl               anaconda\n",
      "    ca-certificates: 2018.4.16-0                   conda-forge --> 2018.03.07-0          anaconda\n",
      "    numpy:           1.14.3-py36_blas_openblas_200 conda-forge [blas_openblas] --> 1.14.3-py36hcd700cb_1 anaconda\n",
      "    scipy:           1.1.0-py36_blas_openblas_200  conda-forge [blas_openblas] --> 1.1.0-py36hfc37229_0  anaconda\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "numpy 1.14.3: ########################################################## | 100% \n",
      "mkl_fft 1.0.1: ######################################################### | 100% \n",
      "ca-certificates 2018.03.07: ############################################ | 100% \n",
      "openssl 1.0.2o: ######################################################## | 100% \n",
      "nltk 3.3.0: ############################################################ | 100% \n",
      "certifi 2018.4.16: ##################################################### | 100% \n",
      "mkl_random 1.0.1: ###################################################### | 100% \n",
      "numpy-base 1.14.3: ##################################################### | 100% \n",
      "libgfortran-ng 7.2.0: ################################################## | 100% \n",
      "blas 1.0: ############################################################## | 100% \n",
      "scipy 1.1.0: ########################################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.4.10\n",
      "  latest version: 4.5.4\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ubuntu/anaconda/envs/kaggle\n",
      "\n",
      "  added / updated specs: \n",
      "    - keras\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    h5py-2.8.0                 |   py36h470a237_0         3.7 MB  conda-forge\n",
      "    certifi-2018.4.16          |           py36_0         142 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.9 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    h5py:            2.8.0-py36h470a237_0 conda-forge\n",
      "    keras:           2.1.5-py36_0         conda-forge\n",
      "    libgpuarray:     0.7.6-0              conda-forge\n",
      "    mako:            1.0.7-py36_0         conda-forge\n",
      "    pygpu:           0.7.6-py36_0         conda-forge\n",
      "    theano:          1.0.2-py36_0         conda-forge\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    ca-certificates: 2018.03.07-0         anaconda    --> 2018.4.16-0      conda-forge\n",
      "    certifi:         2018.4.16-py36_0     anaconda    --> 2018.4.16-py36_0 conda-forge\n",
      "    openssl:         1.0.2o-h20670df_0    anaconda    --> 1.0.2o-0         conda-forge\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "h5py 2.8.0: ############################################################ | 100% \n",
      "certifi 2018.4.16: ##################################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c anaconda -y nltk \n",
    "!conda install -c conda-forge -y keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorlayer\n",
      "  Using cached https://files.pythonhosted.org/packages/30/ec/10a69f280c07b1d49870ddfa034264a5f3ef0a3d6fe8da345fd8c32cbc83/tensorlayer-1.8.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scipy<1.2,>=1.1 in /home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages (from tensorlayer)\n",
      "Requirement already satisfied: matplotlib<2.3,>=2.2 in /home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages (from tensorlayer)\n",
      "Collecting progressbar2<3.38,>=3.37 (from tensorlayer)\n",
      "  Using cached https://files.pythonhosted.org/packages/ca/49/59a86321fafcc2a57ea37925d35f05b20b076941934afd75ce1fd92b3125/progressbar2-3.37.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy<1.15,>=1.14 in /home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages (from tensorlayer)\n",
      "Collecting scikit-image<0.14,>=0.13 (from tensorlayer)\n",
      "  Downloading https://files.pythonhosted.org/packages/60/0e/75fbf63c3b7a14fdbfaf92ca77035c18e90963003031148211bf12441be7/scikit_image-0.13.1-cp36-cp36m-manylinux1_x86_64.whl (35.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 35.8MB 24kB/s \n",
      "\u001b[?25hCollecting imageio<2.4,>=2.3 (from tensorlayer)\n",
      "  Using cached https://files.pythonhosted.org/packages/a7/1d/33c8686072148b3b0fcc12a2e0857dd8316b8ae20a0fa66c8d6a6d01c05c/imageio-2.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib<2.3,>=2.2->tensorlayer)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages (from matplotlib<2.3,>=2.2->tensorlayer)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages (from matplotlib<2.3,>=2.2->tensorlayer)\n",
      "Requirement already satisfied: pytz in /home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages (from matplotlib<2.3,>=2.2->tensorlayer)\n",
      "Requirement already satisfied: six>=1.10 in /home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages (from matplotlib<2.3,>=2.2->tensorlayer)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages (from matplotlib<2.3,>=2.2->tensorlayer)\n",
      "Collecting python-utils>=2.3.0 (from progressbar2<3.38,>=3.37->tensorlayer)\n",
      "  Using cached https://files.pythonhosted.org/packages/eb/a0/19119d8b7c05be49baf6c593f11c432d571b70d805f2fe94c0585e55e4c8/python_utils-2.3.0-py2.py3-none-any.whl\n",
      "Collecting PyWavelets>=0.4.0 (from scikit-image<0.14,>=0.13->tensorlayer)\n",
      "  Downloading https://files.pythonhosted.org/packages/32/c0/3646053c0ce297686da524bc968bff6017151a9089d16c33afe7d330a48b/PyWavelets-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (5.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.7MB 155kB/s \n",
      "\u001b[?25hRequirement already satisfied: pillow>=2.1.0 in /home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages (from scikit-image<0.14,>=0.13->tensorlayer)\n",
      "Collecting networkx>=1.8 (from scikit-image<0.14,>=0.13->tensorlayer)\n",
      "  Downloading https://files.pythonhosted.org/packages/11/42/f951cc6838a4dff6ce57211c4d7f8444809ccbe2134179950301e5c4c83c/networkx-2.1.zip (1.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 572kB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib<2.3,>=2.2->tensorlayer)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages (from networkx>=1.8->scikit-image<0.14,>=0.13->tensorlayer)\n",
      "Building wheels for collected packages: networkx\n",
      "  Running setup.py bdist_wheel for networkx ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/44/c0/34/6f98693a554301bdb405f8d65d95bbcd3e50180cbfdd98a94e\n",
      "Successfully built networkx\n",
      "Installing collected packages: python-utils, progressbar2, PyWavelets, networkx, scikit-image, imageio, tensorlayer\n",
      "Successfully installed PyWavelets-0.5.2 imageio-2.3.0 networkx-2.1 progressbar2-3.37.1 python-utils-2.3.0 scikit-image-0.13.1 tensorlayer-1.8.5\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import pearsonr\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('data/train.csv')\n",
    "test=pd.read_csv('data/test.csv')\n",
    "resources=pd.read_csv('data/resources.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.dtypes)\n",
    "print(test.dtypes)\n",
    "print(resources.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([train,test])\n",
    "df.loc[df.project_essay_3.isna(),['project_essay_2','project_essay_3']]=df.loc[df.project_essay_4.isna(),['project_essay_3','project_essay_2']].values\n",
    "df[['project_essay_2','project_essay_4']]=df[['project_essay_2','project_essay_4']].fillna(\"\")\n",
    "df['project_essay_1']=df.apply(lambda x:x['project_essay_1']+x['project_essay_2'],axis=1)\n",
    "df['project_essay_2']=df.apply(lambda x:x['project_essay_3']+x['project_essay_4'],axis=1)\n",
    "df=df.drop(['project_essay_3','project_essay_4'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources['total_price']=resources['quantity'] * resources['price']\n",
    "\n",
    "R=resources.groupby('id').agg({'description':'count','quantity':'sum','price':'sum','total_price':'sum'})\\\n",
    "    .rename(columns={'description':'items'})\n",
    "R['avg_price']=R['total_price']/R['quantity']\n",
    "\n",
    "for func in ['min','max','mean','std']:\n",
    "    R=R.join(resources.groupby('id').agg({'quantity':func,'price':func,'total_price':func}).\\\n",
    "           rename(columns={'quantity':'quantity_'+func,'price':'price_'+func,'total_price':'total_price_'+func}))\n",
    "\n",
    "R=R.join(resources.groupby('id').agg({'description':lambda x:' '.join(x.astype(str))}).rename(\n",
    "    columns={'description':'resource_description'}))\n",
    "\n",
    "df=df.join(R,on='id')\n",
    "\n",
    "df['price_category']=pd.cut(df['total_price'], [0, 50, 100, 250, 500, 1000,np.inf])\n",
    "\n",
    "for c in ['quantity', 'price', 'total_price']:\n",
    "    df['max%s_min%s'%(c,c)] = df['%s_max'%c] - df['%s_min'%c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['teacher_id'] = le.fit_transform(df['teacher_id'])\n",
    "df['teacher_gender_unknown'] = df.teacher_prefix.apply(lambda x:int(x not in ['Ms.', 'Mrs.', 'Mr.']))\n",
    "\n",
    "statFeatures = []\n",
    "for col in ['school_state', 'teacher_id', 'teacher_prefix', 'teacher_gender_unknown', 'project_grade_category', 'project_subject_categories', 'project_subject_subcategories', 'teacher_number_of_previously_posted_projects']:\n",
    "    Stat = df[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_stat'})\n",
    "    Stat /= Stat.sum()\n",
    "    df = df.join(Stat, on=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "numFeatures=[df.columns[i] for i,j in enumerate(df.dtypes) if j == 'float64' and not (df.columns[i]=='project_is_approved') ]\n",
    "T2 = df[numFeatures+['project_is_approved']].copy()\n",
    "Ttr = T2[-pd.isna(df.project_is_approved)]\n",
    "Tar_tr = Ttr['project_is_approved'].values\n",
    "n = 10\n",
    "inx = [np.random.randint(0, Ttr.shape[0], int(Ttr.shape[0]/n)) for k in range(n)]\n",
    "# inx is used for crossvalidation of calculating the correlation and p-value\n",
    "Corr = {}\n",
    "for c in numFeatures:\n",
    "    # since some values might be 0s, I use x+1 to avoid missing some important relations\n",
    "    C1,P1=np.nanmean([pearsonr(Tar_tr[inx[k]],   (1+Ttr[c].iloc[inx[k]])) for k in range(n)], 0)\n",
    "    C2,P2=np.nanmean([pearsonr(Tar_tr[inx[k]], 1/(1+Ttr[c].iloc[inx[k]])) for k in range(n)], 0)\n",
    "    if P2<P1:\n",
    "        T2[c] = 1/(1+T2[c])\n",
    "        Corr[c] = [C2,P2]\n",
    "    else:\n",
    "        T2[c] = 1+T2[c]\n",
    "        Corr[c] = [C1,P1]\n",
    "        \n",
    "        \n",
    "polyCol = []\n",
    "thrP = 0.01\n",
    "thrC = 0.02\n",
    "print('columns \\t\\t\\t Corr1 \\t\\t Corr2 \\t\\t Corr Combined')\n",
    "for i, c1 in enumerate(numFeatures[:-1]):\n",
    "    C1, P1 = Corr[c1]\n",
    "    for c2 in numFeatures[i+1:]:\n",
    "        C2, P2 = Corr[c2]\n",
    "        V = T2[c1] * T2[c2]\n",
    "        Vtr = V[-pd.isna(T2.project_is_approved)].values\n",
    "        C, P = np.nanmean([pearsonr(Tar_tr[inx[k]], Vtr[inx[k]]) for k in range(n)], 0)\n",
    "        if P<thrP and abs(C) - max(abs(C1),abs(C2)) > thrC:\n",
    "            df[c1+'_'+c2+'_poly'] = V\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateCol = 'project_submitted_datetime'\n",
    "def getTimeFeatures(T):\n",
    "    df['year'] = df[dateCol].apply(lambda x: x.year)\n",
    "    df['month'] = df[dateCol].apply(lambda x: x.month)\n",
    "    df['day'] = df[dateCol].apply(lambda x: x.day)\n",
    "    df['dow'] = df[dateCol].apply(lambda x: x.dayofweek)\n",
    "    df['hour'] = df[dateCol].apply(lambda x: x.hour)\n",
    "    df['days'] = (df[dateCol]-df[dateCol].min()).apply(lambda x: x.days)\n",
    "    return T\n",
    "\n",
    "df[dateCol] = pd.to_datetime(df[dateCol])\n",
    "df = getTimeFeatures(df)\n",
    "\n",
    "timeFeatures = ['year', 'month', 'day', 'dow', 'hour', 'days']\n",
    "for col in timeFeatures:\n",
    "    Stat = df[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_stat'})\n",
    "    Stat /= Stat.sum()\n",
    "    df = df.join(Stat, on=col)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCatFeatures(T, Col):\n",
    "    vectorizer = CountVectorizer(binary=True,\n",
    "                                 ngram_range=(1,1),\n",
    "                                 tokenizer=lambda x:[a.strip() for a in x.split(',')])\n",
    "    return vectorizer.fit_transform(T[Col].fillna(''))\n",
    "\n",
    "X_tp = getCatFeatures(df, 'teacher_prefix')\n",
    "X_ss = getCatFeatures(df, 'school_state')\n",
    "X_pgc = getCatFeatures(df, 'project_grade_category')\n",
    "X_psc = getCatFeatures(df, 'project_subject_categories')\n",
    "X_pssc = getCatFeatures(df, 'project_subject_subcategories')\n",
    "\n",
    "X_cat = sparse.hstack((X_tp, X_ss, X_pgc, X_psc, X_pssc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PorterStemmer()\n",
    "def wordPreProcess(sentence):\n",
    "    return ' '.join([p.stem(x.lower()) for x in re.split('\\W', sentence) if len(x) >= 1])\n",
    "\n",
    "\n",
    "\n",
    "def getTextFeatures(T, Col, max_features=10000, ngrams=(1,2), verbose=True):\n",
    "    if verbose:\n",
    "        print('processing: ', Col)\n",
    "    vectorizer = CountVectorizer(stop_words=None,\n",
    "                                 preprocessor=wordPreProcess,\n",
    "                                 max_features=max_features,\n",
    "                                 binary=True,\n",
    "                                 ngram_range=ngrams)\n",
    "    X = vectorizer.fit_transform(T[Col])\n",
    "    return X, vectorizer.get_feature_names()\n",
    "\n",
    "n_es1, n_es2, n_prs, n_rd, n_pt = 3000, 8000, 2000, 3000, 1000\n",
    "X_es1, feat_es1 = getTextFeatures(df, 'project_essay_1', max_features=n_es1)\n",
    "X_es2, feat_es2 = getTextFeatures(df, 'project_essay_2', max_features=n_es2)\n",
    "X_prs, feat_prs = getTextFeatures(df, 'project_resource_summary', max_features=n_prs)\n",
    "X_rd, feat_rd = getTextFeatures(df, 'resource_description', max_features=n_rd, ngrams=(1,3))\n",
    "X_pt, feat_pt = getTextFeatures(df, 'project_title', max_features=n_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_txt = sparse.hstack((X_es1, X_es2, X_prs, X_rd, X_pt))\n",
    "del X_es1, X_es2, X_prs, X_rd, X_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_str(string):\n",
    "    string = re.sub(r'(\\\")', ' ', string)\n",
    "    string = re.sub(r'(\\r)', ' ', string)\n",
    "    string = re.sub(r'(\\n)', ' ', string)\n",
    "    string = re.sub(r'(\\r\\n)', ' ', string)\n",
    "    string = re.sub(r'(\\\\)', ' ', string)\n",
    "    string = re.sub(r'\\t', ' ', string)\n",
    "    string = re.sub(r'\\:', ' ', string)\n",
    "    string = re.sub(r'\\\"\\\"\\\"\\\"', ' ', string)\n",
    "    string = re.sub(r'_', ' ', string)\n",
    "    string = re.sub(r'\\+', ' ', string)\n",
    "    string = re.sub(r'\\=', ' ', string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path='data/df.p'\n",
    "if os.path.exists(df_path):\n",
    "    df=pd.read_pickle('data/df.p')\n",
    "else:\n",
    "    df.to_pickle('data/df.p')\n",
    "    \n",
    "txt_path = 'data/txt.npy'\n",
    "if os.path.exists(txt_path):\n",
    "    X_txt = pickle.load(open(txt_path, 'rb'))\n",
    "else:\n",
    "    pickle.dump(X_txt, open(txt_path, 'wb'))\n",
    "\n",
    "cat_path = 'data/cat.npy'\n",
    "if os.path.exists(cat_path):\n",
    "    X_cat = pickle.load(open(cat_path, 'rb'))\n",
    "else:\n",
    "    pickle.dump(X_cat, open(cat_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "numFeatures=[df.columns[i] for i,j in enumerate(df.dtypes) if j == 'float64' and not (df.columns[i]=='project_is_approved') ]\n",
    "X = sparse.hstack((X_txt, X_cat, StandardScaler().fit_transform(df[numFeatures].fillna(0)))).tocsr()\n",
    "\n",
    "Xtr = X[np.argwhere(-pd.isna(df.project_is_approved)).reshape(-1,)]\n",
    "Xts = X[np.argwhere(pd.isna(df.project_is_approved)).reshape(-1,)]\n",
    "Ttr_tar = df[-pd.isna(df.project_is_approved)]['project_is_approved'].values\n",
    "Tts = df[-pd.isna(df.project_is_approved)][['id','project_is_approved']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 154768 samples, validate on 27312 samples\n",
      "Epoch 1/1\n",
      " - 107s - loss: 0.4142 - binary_accuracy: 0.8424 - val_loss: 0.3700 - val_binary_accuracy: 0.8480\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37002, saving model to NN.h5\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Flatten, concatenate, Dropout, Embedding, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "n_es1, n_es2, n_prs, n_rd, n_pt = 3000, 8000, 2000, 3000, 1000\n",
    "def breakInput(X1):\n",
    "    X2 = []\n",
    "    i = 0\n",
    "    for n in [n_es1, n_es2, n_prs, n_rd, n_pt, X_cat.shape[1], len(numFeatures)]:\n",
    "        X2.append(X1[:,i:i+n])\n",
    "        i += n\n",
    "    return X2\n",
    "\n",
    "def getModel(HLs, Drop=0.25, OP=optimizers.Adam()):\n",
    "    temp = []\n",
    "    inputs_txt = []\n",
    "    for n in [n_es1, n_es2, n_prs, n_rd, n_pt]:\n",
    "        input_txt = Input((n, ))\n",
    "        X_feat = Dropout(Drop)(input_txt)\n",
    "        X_feat = Dense(int(n/100), activation=\"linear\")(X_feat)\n",
    "        X_feat = Dropout(Drop)(X_feat)\n",
    "        temp.append(X_feat)\n",
    "        inputs_txt.append(input_txt)\n",
    "\n",
    "    x_1 = concatenate(temp)\n",
    "#     x_1 = Dense(20, activation=\"relu\")(x_1)\n",
    "    x_1 = Dense(50, activation=\"relu\")(x_1)\n",
    "    x_1 = Dropout(Drop)(x_1)\n",
    "\n",
    "    input_cat = Input((X_cat.shape[1], ))\n",
    "    x_2 = Embedding(2, 10, input_length=X_cat.shape[1])(input_cat)\n",
    "    x_2 = SpatialDropout1D(Drop)(x_2)\n",
    "    x_2 = Flatten()(x_2)\n",
    "\n",
    "    input_num = Input((len(numFeatures), ))\n",
    "    x_3 = Dropout(Drop)(input_num)\n",
    "    \n",
    "    x = concatenate([x_1, x_2, x_3])\n",
    "\n",
    "    for HL in HLs:\n",
    "        x = Dense(HL, activation=\"relu\")(x)\n",
    "        x = Dropout(Drop)(x)\n",
    "\n",
    "    output = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs_txt+[input_cat, input_num], outputs=output)\n",
    "    model.compile(\n",
    "            optimizer=OP,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "def trainNN(X_train, X_val, Tar_train, Tar_val, HL=[50], Drop=0.5, OP=optimizers.Adam()):\n",
    "    file_path='NN.h5'\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=6)\n",
    "    lr_reduced = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                   factor=0.5,\n",
    "                                   patience=2,\n",
    "                                   verbose=1,\n",
    "                                   epsilon=3e-4,\n",
    "                                   mode='min')\n",
    "\n",
    "    model = getModel(HL, Drop, OP)\n",
    "    model.fit(breakInput(X_train), Tar_train, validation_data=(breakInput(X_val), Tar_val),\n",
    "                        verbose=2, epochs=1, batch_size=1000, callbacks=[early, lr_reduced, checkpoint])\n",
    "    model.load_weights(file_path)\n",
    "    return model\n",
    "\n",
    "nCV = 1 # should be ideally larger\n",
    "for i in range(21, 22):\n",
    "    X_train, X_val, Tar_train, Tar_val = train_test_split(Xtr, Ttr_tar, test_size=0.15, random_state=i, stratify=Ttr_tar)\n",
    "    model = trainNN(X_train, X_val, Tar_train, Tar_val, HL=[50], Drop=0.5, OP=optimizers.Adam())\n",
    "    Yvl3 = model.predict(breakInput(X_val)).squeeze()\n",
    "    Yts3 = model.predict(breakInput(Xts)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 8000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 2000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 3000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 3000)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 8000)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 2000)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 3000)         0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1000)         0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30)           90030       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 80)           640080      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 20)           40020       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 30)           90030       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           10010       dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 30)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 80)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 30)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 10)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 170)          0           dropout_2[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 10)      20          input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 50)           8550        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 100, 10)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 37)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 50)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1000)         0           spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 37)           0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1087)         0           dropout_11[0][0]                 \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 50)           54400       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 50)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            51          dropout_13[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 933,191\n",
      "Trainable params: 933,191\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'model_1',\n",
       " 'layers': [{'name': 'input_1',\n",
       "   'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 3000),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'name': 'input_1'},\n",
       "   'inbound_nodes': []},\n",
       "  {'name': 'input_2',\n",
       "   'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 8000),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'name': 'input_2'},\n",
       "   'inbound_nodes': []},\n",
       "  {'name': 'input_3',\n",
       "   'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 2000),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'name': 'input_3'},\n",
       "   'inbound_nodes': []},\n",
       "  {'name': 'input_4',\n",
       "   'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 3000),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'name': 'input_4'},\n",
       "   'inbound_nodes': []},\n",
       "  {'name': 'input_5',\n",
       "   'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 1000),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'name': 'input_5'},\n",
       "   'inbound_nodes': []},\n",
       "  {'name': 'dropout_1',\n",
       "   'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_1',\n",
       "    'trainable': True,\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None},\n",
       "   'inbound_nodes': [[['input_1', 0, 0, {}]]]},\n",
       "  {'name': 'dropout_3',\n",
       "   'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_3',\n",
       "    'trainable': True,\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None},\n",
       "   'inbound_nodes': [[['input_2', 0, 0, {}]]]},\n",
       "  {'name': 'dropout_5',\n",
       "   'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_5',\n",
       "    'trainable': True,\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None},\n",
       "   'inbound_nodes': [[['input_3', 0, 0, {}]]]},\n",
       "  {'name': 'dropout_7',\n",
       "   'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_7',\n",
       "    'trainable': True,\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None},\n",
       "   'inbound_nodes': [[['input_4', 0, 0, {}]]]},\n",
       "  {'name': 'dropout_9',\n",
       "   'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_9',\n",
       "    'trainable': True,\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None},\n",
       "   'inbound_nodes': [[['input_5', 0, 0, {}]]]},\n",
       "  {'name': 'dense_1',\n",
       "   'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_1',\n",
       "    'trainable': True,\n",
       "    'units': 30,\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['dropout_1', 0, 0, {}]]]},\n",
       "  {'name': 'dense_2',\n",
       "   'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_2',\n",
       "    'trainable': True,\n",
       "    'units': 80,\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['dropout_3', 0, 0, {}]]]},\n",
       "  {'name': 'dense_3',\n",
       "   'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_3',\n",
       "    'trainable': True,\n",
       "    'units': 20,\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['dropout_5', 0, 0, {}]]]},\n",
       "  {'name': 'dense_4',\n",
       "   'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_4',\n",
       "    'trainable': True,\n",
       "    'units': 30,\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['dropout_7', 0, 0, {}]]]},\n",
       "  {'name': 'dense_5',\n",
       "   'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_5',\n",
       "    'trainable': True,\n",
       "    'units': 10,\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['dropout_9', 0, 0, {}]]]},\n",
       "  {'name': 'dropout_2',\n",
       "   'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_2',\n",
       "    'trainable': True,\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None},\n",
       "   'inbound_nodes': [[['dense_1', 0, 0, {}]]]},\n",
       "  {'name': 'dropout_4',\n",
       "   'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_4',\n",
       "    'trainable': True,\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None},\n",
       "   'inbound_nodes': [[['dense_2', 0, 0, {}]]]},\n",
       "  {'name': 'dropout_6',\n",
       "   'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_6',\n",
       "    'trainable': True,\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None},\n",
       "   'inbound_nodes': [[['dense_3', 0, 0, {}]]]},\n",
       "  {'name': 'dropout_8',\n",
       "   'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_8',\n",
       "    'trainable': True,\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None},\n",
       "   'inbound_nodes': [[['dense_4', 0, 0, {}]]]},\n",
       "  {'name': 'dropout_10',\n",
       "   'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_10',\n",
       "    'trainable': True,\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None},\n",
       "   'inbound_nodes': [[['dense_5', 0, 0, {}]]]},\n",
       "  {'name': 'input_6',\n",
       "   'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 100),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'name': 'input_6'},\n",
       "   'inbound_nodes': []},\n",
       "  {'name': 'concatenate_1',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'concatenate_1', 'trainable': True, 'axis': -1},\n",
       "   'inbound_nodes': [[['dropout_2', 0, 0, {}],\n",
       "     ['dropout_4', 0, 0, {}],\n",
       "     ['dropout_6', 0, 0, {}],\n",
       "     ['dropout_8', 0, 0, {}],\n",
       "     ['dropout_10', 0, 0, {}]]]},\n",
       "  {'name': 'embedding_1',\n",
       "   'class_name': 'Embedding',\n",
       "   'config': {'name': 'embedding_1',\n",
       "    'trainable': True,\n",
       "    'batch_input_shape': (None, 100),\n",
       "    'dtype': 'float32',\n",
       "    'input_dim': 2,\n",
       "    'output_dim': 10,\n",
       "    'embeddings_initializer': {'class_name': 'RandomUniform',\n",
       "     'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}},\n",
       "    'embeddings_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'embeddings_constraint': None,\n",
       "    'mask_zero': False,\n",
       "    'input_length': 100},\n",
       "   'inbound_nodes': [[['input_6', 0, 0, {}]]]},\n",
       "  {'name': 'dense_6',\n",
       "   'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_6',\n",
       "    'trainable': True,\n",
       "    'units': 50,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['concatenate_1', 0, 0, {}]]]},\n",
       "  {'name': 'spatial_dropout1d_1',\n",
       "   'class_name': 'SpatialDropout1D',\n",
       "   'config': {'name': 'spatial_dropout1d_1',\n",
       "    'trainable': True,\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None},\n",
       "   'inbound_nodes': [[['embedding_1', 0, 0, {}]]]},\n",
       "  {'name': 'input_7',\n",
       "   'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 37),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'name': 'input_7'},\n",
       "   'inbound_nodes': []},\n",
       "  {'name': 'dropout_11',\n",
       "   'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_11',\n",
       "    'trainable': True,\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None},\n",
       "   'inbound_nodes': [[['dense_6', 0, 0, {}]]]},\n",
       "  {'name': 'flatten_1',\n",
       "   'class_name': 'Flatten',\n",
       "   'config': {'name': 'flatten_1', 'trainable': True},\n",
       "   'inbound_nodes': [[['spatial_dropout1d_1', 0, 0, {}]]]},\n",
       "  {'name': 'dropout_12',\n",
       "   'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_12',\n",
       "    'trainable': True,\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None},\n",
       "   'inbound_nodes': [[['input_7', 0, 0, {}]]]},\n",
       "  {'name': 'concatenate_2',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'concatenate_2', 'trainable': True, 'axis': -1},\n",
       "   'inbound_nodes': [[['dropout_11', 0, 0, {}],\n",
       "     ['flatten_1', 0, 0, {}],\n",
       "     ['dropout_12', 0, 0, {}]]]},\n",
       "  {'name': 'dense_7',\n",
       "   'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_7',\n",
       "    'trainable': True,\n",
       "    'units': 50,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['concatenate_2', 0, 0, {}]]]},\n",
       "  {'name': 'dropout_13',\n",
       "   'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_13',\n",
       "    'trainable': True,\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None},\n",
       "   'inbound_nodes': [[['dense_7', 0, 0, {}]]]},\n",
       "  {'name': 'dense_8',\n",
       "   'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_8',\n",
       "    'trainable': True,\n",
       "    'units': 1,\n",
       "    'activation': 'sigmoid',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['dropout_13', 0, 0, {}]]]}],\n",
       " 'input_layers': [['input_1', 0, 0],\n",
       "  ['input_2', 0, 0],\n",
       "  ['input_3', 0, 0],\n",
       "  ['input_4', 0, 0],\n",
       "  ['input_5', 0, 0],\n",
       "  ['input_6', 0, 0],\n",
       "  ['input_7', 0, 0]],\n",
       " 'output_layers': [['dense_8', 0, 0]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_batch_generator(X_data, y_data, batch_size,shuffle=True):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    if shuffle:\n",
    "        index = np.arange(np.shape(y_data)[0])\n",
    "        np.random.shuffle(index)\n",
    "        X =  X_data[index, :]\n",
    "        y =  y_data[index]\n",
    "    else:\n",
    "        index = np.arange(np.shape(y_data)[0])\n",
    "    while counter < number_of_batches:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].todense()\n",
    "        y_batch = y_data[index_batch]\n",
    "        counter += 1\n",
    "        yield np.array(X_batch),y_batch\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] InputLayer  n_es1_input_layer: (?, 3000)\n",
      "[TL] DropoutLayer n_es1_drop1: keep:0.500000 is_fix:False\n",
      "[TL] DenseLayer  n_es1_dense: 30 identity\n",
      "[TL] DropoutLayer n_es1drop2: keep:0.500000 is_fix:False\n",
      "[TL] InputLayer  n_es2_input_layer: (?, 8000)\n",
      "[TL] DropoutLayer n_es2_drop1: keep:0.500000 is_fix:False\n",
      "[TL] DenseLayer  n_es2_dense: 80 identity\n",
      "[TL] DropoutLayer n_es2drop2: keep:0.500000 is_fix:False\n",
      "[TL] InputLayer  n_prs_input_layer: (?, 2000)\n",
      "[TL] DropoutLayer n_prs_drop1: keep:0.500000 is_fix:False\n",
      "[TL] DenseLayer  n_prs_dense: 20 identity\n",
      "[TL] DropoutLayer n_prsdrop2: keep:0.500000 is_fix:False\n",
      "[TL] InputLayer  n_rd_input_layer: (?, 3000)\n",
      "[TL] DropoutLayer n_rd_drop1: keep:0.500000 is_fix:False\n",
      "[TL] DenseLayer  n_rd_dense: 30 identity\n",
      "[TL] DropoutLayer n_rddrop2: keep:0.500000 is_fix:False\n",
      "[TL] InputLayer  n_pt_input_layer: (?, 1000)\n",
      "[TL] DropoutLayer n_pt_drop1: keep:0.500000 is_fix:False\n",
      "[TL] DenseLayer  n_pt_dense: 10 identity\n",
      "[TL] DropoutLayer n_ptdrop2: keep:0.500000 is_fix:False\n",
      "[TL] ConcatLayer input_concat_layer: axis: 1\n",
      "[TL] DenseLayer  n_pt: 50 identity\n",
      "[TL] DropoutLayer input_concat_layer_drop: keep:0.500000 is_fix:False\n",
      "[TL] EmbeddingInputlayer x2_embed: (2, 10)\n",
      "[TL] FlattenLayer flatten: 1000\n",
      "[TL] InputLayer  x3_layer: (?, 37)\n",
      "[TL] DropoutLayer x3_embed_drop: keep:0.500000 is_fix:False\n",
      "[TL] ConcatLayer concat_layer: axis: -1\n",
      "[TL] DenseLayer  x_dense_50: 50 relu\n",
      "[TL] DropoutLayer x_drop_50: keep:0.500000 is_fix:False\n",
      "[TL] DenseLayer  output_layer: 1 sigmoid\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor conversion requested dtype int64 for Tensor with dtype float32: 'Tensor(\"cross_entropy/Log:0\", shape=(?, 1), dtype=float32)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-91ff95fa3322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cross_entropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mtrain_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/kaggle/lib/python3.6/site-packages/tensorlayer/cost.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(output, target, epsilon, name)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         return tf.reduce_mean(\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/kaggle/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    968\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m           \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m           \u001b[0;31m# If the RHS is not a tensor, it might be a tensor aware object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/kaggle/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m   1012\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/kaggle/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/kaggle/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[0;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    945\u001b[0m     raise ValueError(\n\u001b[1;32m    946\u001b[0m         \u001b[0;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[1;32m    948\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype int64 for Tensor with dtype float32: 'Tensor(\"cross_entropy/Log:0\", shape=(?, 1), dtype=float32)'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "HLs=[50]\n",
    "drop=0.5\n",
    "learning_rate=0.001\n",
    "tf.reset_default_graph()\n",
    "placholder={}\n",
    "input_net={}\n",
    "# define placeholder\n",
    "for n,label in zip([n_es1, n_es2, n_prs, n_rd, n_pt],['n_es1', 'n_es2', 'n_prs', 'n_rd', 'n_pt']):\n",
    "    placholder[label] = tf.placeholder(tf.float32, shape=[None, n], name=label)\n",
    "    input_net[label] = tl.layers.InputLayer(placholder[label], name=label +'_input_layer')\n",
    "    input_net[label] = tl.layers.DropoutLayer(input_net[label], keep=drop, name=label+'_drop1')\n",
    "    input_net[label] = tl.layers.DenseLayer(input_net[label], n_units=int(n/100), name=label+'_dense')\n",
    "    input_net[label] = tl.layers.DropoutLayer(input_net[label], keep=drop, name=label+'drop2')\n",
    "    \n",
    "x2_placeholder= tf.placeholder(tf.int64, shape=[None, X_cat.shape[1]], name='x2')\n",
    "\n",
    "input_num=len(numFeatures)\n",
    "x3_placeholder = tf.placeholder(tf.float32, shape=[None, input_num], name='x3')\n",
    "\n",
    "y_ = tf.placeholder(tf.int64, shape=[None, ], name='target')\n",
    "\n",
    "x1=tl.layers.ConcatLayer([input_net[key] for key in input_net], 1, name ='input_concat_layer')\n",
    "x1=tl.layers.DenseLayer(x1, n_units=50., name=label)\n",
    "x1=tl.layers.DropoutLayer(x1, keep=drop, name='input_concat_layer_drop')\n",
    "\n",
    "x2=tl.layers.EmbeddingInputlayer(inputs=x2_placeholder, vocabulary_size=2, embedding_size=10, name='x2_embed')\n",
    "#x2=tl.layers.DropoutLayer(x2, keep=drop, name=label+'embed_drop')\n",
    "x2 = tl.layers.FlattenLayer(x2)\n",
    "\n",
    "x3 = tl.layers.InputLayer(x3_placeholder, name='x3_layer')\n",
    "x3 = tl.layers.DropoutLayer(x3, keep=drop, name='x3_embed_drop')\n",
    "\n",
    "x = tl.layers.ConcatLayer([x1,x2,x3])\n",
    "\n",
    "for HL in HLs:\n",
    "    x = tl.layers.DenseLayer(x, n_units=50.,act=tf.nn.relu, name='x_dense_'+str(HL))\n",
    "    x = tl.layers.DropoutLayer(x, keep=drop, name='x_drop_'+str(HL))\n",
    "\n",
    "x = tl.layers.DenseLayer(x, n_units=1,\n",
    "                                act = tf.nn.sigmoid,\n",
    "                                name='output_layer')\n",
    "\n",
    "y = x.outputs\n",
    "cost = tl.cost.binary_cross_entropy(y, y_,name='cross_entropy')\n",
    "\n",
    "train_params = x.all_params\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,\n",
    "                            epsilon=1e-08, use_locking=False).minimize(cost, var_list = train_params)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tl.layers.initialize_global_variables(sess)\n",
    "\n",
    "n_epoch =50\n",
    "batch_size = 50\n",
    "\n",
    "X_train, X_val, Tar_train, Tar_val = train_test_split(Xtr, Ttr_tar, test_size=0.15, random_state=i, stratify=Ttr_tar)\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    batch=nn_batch_generator(X_train, Tar_train,1000)\n",
    "    for x_t,y_t in batch:\n",
    "        x_t_es1, x_t_es2, x_t_prs, x_t_rd, x_t_pt,x_t_x2,x_t_x3 = breakInput(x_t)\n",
    "        y_t_batch = y_t\n",
    "        feed_dict={}\n",
    "        \n",
    "        feed_dict[placholder['n_es1']]=x_t_es1\n",
    "        feed_dict[placholder['n_es2']]=x_t_es2\n",
    "        feed_dict[placholder['n_prs']]=x_t_prs\n",
    "        feed_dict[placholder['n_rd']]=x_t_rd\n",
    "        feed_dict[placholder['n_pt']]=x_t_pt\n",
    "        feed_dict[x2_placeholder]=x_t_x2\n",
    "        feed_dict[x3_placeholder]=x_t_x3\n",
    "        feed_dict[y_]=y_t_batch\n",
    "        \n",
    "        feed_dict.update(x.all_drop)  # enable noise layers\n",
    "        sess.run(train_op, feed_dict=feed_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
