{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import pearsonr\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import pylab as pl\n",
    "import textacy\n",
    "import en_core_web_md\n",
    "from textacy import keyterms\n",
    "from textacy import similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('data/train.csv')\n",
    "test=pd.read_csv('data/test.csv')\n",
    "resources=pd.read_csv('data/resources.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                              object\n",
      "teacher_id                                      object\n",
      "teacher_prefix                                  object\n",
      "school_state                                    object\n",
      "project_submitted_datetime                      object\n",
      "project_grade_category                          object\n",
      "project_subject_categories                      object\n",
      "project_subject_subcategories                   object\n",
      "project_title                                   object\n",
      "project_essay_1                                 object\n",
      "project_essay_2                                 object\n",
      "project_essay_3                                 object\n",
      "project_essay_4                                 object\n",
      "project_resource_summary                        object\n",
      "teacher_number_of_previously_posted_projects     int64\n",
      "project_is_approved                              int64\n",
      "dtype: object\n",
      "id                                              object\n",
      "teacher_id                                      object\n",
      "teacher_prefix                                  object\n",
      "school_state                                    object\n",
      "project_submitted_datetime                      object\n",
      "project_grade_category                          object\n",
      "project_subject_categories                      object\n",
      "project_subject_subcategories                   object\n",
      "project_title                                   object\n",
      "project_essay_1                                 object\n",
      "project_essay_2                                 object\n",
      "project_essay_3                                 object\n",
      "project_essay_4                                 object\n",
      "project_resource_summary                        object\n",
      "teacher_number_of_previously_posted_projects     int64\n",
      "dtype: object\n",
      "id              object\n",
      "description     object\n",
      "quantity         int64\n",
      "price          float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train.dtypes)\n",
    "print(test.dtypes)\n",
    "print(resources.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df=pd.concat([train,test])\n",
    "df.loc[df.project_essay_3.isna(),['project_essay_2','project_essay_3']]=df.loc[df.project_essay_4.isna(),['project_essay_3','project_essay_2']].values\n",
    "df[['project_essay_2','project_essay_4']]=df[['project_essay_2','project_essay_4']].fillna(\"\")\n",
    "df['project_essay_1']=df.apply(lambda x:x['project_essay_1']+x['project_essay_2'],axis=1)\n",
    "df['project_essay_2']=df.apply(lambda x:x['project_essay_3']+x['project_essay_4'],axis=1)\n",
    "df=df.drop(['project_essay_3','project_essay_4'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources['total_price']=resources['quantity'] * resources['price']\n",
    "\n",
    "R=resources.groupby('id').agg({'description':'count','quantity':'sum','price':'sum','total_price':'sum'})\\\n",
    "    .rename(columns={'description':'items'})\n",
    "R['avg_price']=R['total_price']/R['quantity']\n",
    "\n",
    "for func in ['min','max','mean','std']:\n",
    "    R=R.join(resources.groupby('id').agg({'quantity':func,'price':func,'total_price':func}).\\\n",
    "           rename(columns={'quantity':'quantity_'+func,'price':'price_'+func,'total_price':'total_price_'+func}))\n",
    "\n",
    "R=R.join(resources.groupby('id').agg({'description':lambda x:' '.join(x.astype(str))}).rename(\n",
    "    columns={'description':'resource_description'}))\n",
    "\n",
    "df=df.join(R,on='id')\n",
    "\n",
    "df['price_category']=pd.cut(df['total_price'], [0, 50, 100, 250, 500, 1000,np.inf])\n",
    "\n",
    "for c in ['quantity', 'price', 'total_price']:\n",
    "    df['max%s_min%s'%(c,c)] = df['%s_max'%c] - df['%s_min'%c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['teacher_id'] = le.fit_transform(df['teacher_id'])\n",
    "df['teacher_gender_unknown'] = df.teacher_prefix.apply(lambda x:int(x not in ['Ms.', 'Mrs.', 'Mr.']))\n",
    "\n",
    "statFeatures = []\n",
    "for col in ['school_state', 'teacher_id', 'teacher_prefix', 'teacher_gender_unknown', 'project_grade_category', 'project_subject_categories', 'project_subject_subcategories', 'teacher_number_of_previously_posted_projects']:\n",
    "    Stat = df[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_stat'})\n",
    "    Stat /= Stat.sum()\n",
    "    df = df.join(Stat, on=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: Mean of empty slice\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: Mean of empty slice\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "numFeatures=[df.columns[i] for i,j in enumerate(df.dtypes) if j == 'float64' and not (df.columns[i]=='project_is_approved') ]\n",
    "T2 = df[numFeatures+['project_is_approved']].copy()\n",
    "Ttr = T2[-pd.isna(df.project_is_approved)]\n",
    "Tar_tr = Ttr['project_is_approved'].values\n",
    "n = 10\n",
    "inx = [np.random.randint(0, Ttr.shape[0], int(Ttr.shape[0]/n)) for k in range(n)]\n",
    "# inx is used for crossvalidation of calculating the correlation and p-value\n",
    "Corr = {}\n",
    "for c in numFeatures:\n",
    "    # since some values might be 0s, I use x+1 to avoid missing some important relations\n",
    "    C1,P1=np.nanmean([pearsonr(Tar_tr[inx[k]],   (1+Ttr[c].iloc[inx[k]])) for k in range(n)], 0)\n",
    "    C2,P2=np.nanmean([pearsonr(Tar_tr[inx[k]], 1/(1+Ttr[c].iloc[inx[k]])) for k in range(n)], 0)\n",
    "    if P2<P1:\n",
    "        T2[c] = 1/(1+T2[c])\n",
    "        Corr[c] = [C2,P2]\n",
    "    else:\n",
    "        T2[c] = 1+T2[c]\n",
    "        Corr[c] = [C1,P1]\n",
    "        \n",
    "        \n",
    "polyCol = []\n",
    "thrP = 0.01\n",
    "thrC = 0.02\n",
    "print('columns \\t\\t\\t Corr1 \\t\\t Corr2 \\t\\t Corr Combined')\n",
    "for i, c1 in enumerate(numFeatures[:-1]):\n",
    "    C1, P1 = Corr[c1]\n",
    "    for c2 in numFeatures[i+1:]:\n",
    "        C2, P2 = Corr[c2]\n",
    "        V = T2[c1] * T2[c2]\n",
    "        Vtr = V[-pd.isna(T2.project_is_approved)].values\n",
    "        C, P = np.nanmean([pearsonr(Tar_tr[inx[k]], Vtr[inx[k]]) for k in range(n)], 0)\n",
    "        if P<thrP and abs(C) - max(abs(C1),abs(C2)) > thrC:\n",
    "            df[c1+'_'+c2+'_poly'] = V\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateCol = 'project_submitted_datetime'\n",
    "def getTimeFeatures(T):\n",
    "    df['year'] = df[dateCol].apply(lambda x: x.year)\n",
    "    df['month'] = df[dateCol].apply(lambda x: x.month)\n",
    "    df['day'] = df[dateCol].apply(lambda x: x.day)\n",
    "    df['dow'] = df[dateCol].apply(lambda x: x.dayofweek)\n",
    "    df['hour'] = df[dateCol].apply(lambda x: x.hour)\n",
    "    df['days'] = (df[dateCol]-df[dateCol].min()).apply(lambda x: x.days)\n",
    "    return T\n",
    "\n",
    "df[dateCol] = pd.to_datetime(df[dateCol])\n",
    "df = getTimeFeatures(df)\n",
    "\n",
    "timeFeatures = ['year', 'month', 'day', 'dow', 'hour', 'days']\n",
    "for col in timeFeatures:\n",
    "    Stat = df[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_stat'})\n",
    "    Stat /= Stat.sum()\n",
    "    df = df.join(Stat, on=col)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCatFeatures(T, Col):\n",
    "    vectorizer = CountVectorizer(binary=True,\n",
    "                                 ngram_range=(1,1),\n",
    "                                 tokenizer=lambda x:[a.strip() for a in x.split(',')])\n",
    "    return vectorizer.fit_transform(T[Col].fillna(''))\n",
    "\n",
    "X_tp = getCatFeatures(df, 'teacher_prefix')\n",
    "X_ss = getCatFeatures(df, 'school_state')\n",
    "X_pgc = getCatFeatures(df, 'project_grade_category')\n",
    "X_psc = getCatFeatures(df, 'project_subject_categories')\n",
    "X_pssc = getCatFeatures(df, 'project_subject_subcategories')\n",
    "\n",
    "X_cat = sparse.hstack((X_tp, X_ss, X_pgc, X_psc, X_pssc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PorterStemmer()\n",
    "def wordPreProcess(sentence):\n",
    "    return ' '.join([p.stem(x.lower()) for x in re.split('\\W', sentence) if len(x) >= 1])\n",
    "\n",
    "\n",
    "\n",
    "def getTextFeatures(T, Col, max_features=10000, ngrams=(1,2), verbose=True):\n",
    "    if verbose:\n",
    "        print('processing: ', Col)\n",
    "    vectorizer = CountVectorizer(stop_words=None,\n",
    "                                 preprocessor=wordPreProcess,\n",
    "                                 max_features=max_features,\n",
    "                                 binary=True,\n",
    "                                 ngram_range=ngrams)\n",
    "    X = vectorizer.fit_transform(T[Col])\n",
    "    return X, vectorizer.get_feature_names()\n",
    "\n",
    "n_es1, n_es2, n_prs, n_rd, n_pt = 3000, 8000, 2000, 3000, 1000\n",
    "X_es1, feat_es1 = getTextFeatures(df, 'project_essay_1', max_features=n_es1)\n",
    "X_es2, feat_es2 = getTextFeatures(df, 'project_essay_2', max_features=n_es2)\n",
    "X_prs, feat_prs = getTextFeatures(df, 'project_resource_summary', max_features=n_prs)\n",
    "X_rd, feat_rd = getTextFeatures(df, 'resource_description', max_features=n_rd, ngrams=(1,3))\n",
    "X_pt, feat_pt = getTextFeatures(df, 'project_title', max_features=n_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_txt = sparse.hstack((X_es1, X_es2, X_prs, X_rd, X_pt))\n",
    "del X_es1, X_es2, X_prs, X_rd, X_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_str(string):\n",
    "    string = re.sub(r'(\\\")', ' ', string)\n",
    "    string = re.sub(r'(\\r)', ' ', string)\n",
    "    string = re.sub(r'(\\n)', ' ', string)\n",
    "    string = re.sub(r'(\\r\\n)', ' ', string)\n",
    "    string = re.sub(r'(\\\\)', ' ', string)\n",
    "    string = re.sub(r'\\t', ' ', string)\n",
    "    string = re.sub(r'\\:', ' ', string)\n",
    "    string = re.sub(r'\\\"\\\"\\\"\\\"', ' ', string)\n",
    "    string = re.sub(r'_', ' ', string)\n",
    "    string = re.sub(r'\\+', ' ', string)\n",
    "    string = re.sub(r'\\=', ' ', string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay=df['project_essay_1'].tolist()+df['project_essay_2'].tolist()\n",
    "nlp=en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = textacy.Corpus(nlp, texts = [\"essay\",\"hello stupid\",\"fly cool\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = textacy.Vectorizer(\n",
    "     tf_type='linear', apply_idf=True, idf_type='smooth', norm='l2',\n",
    "     min_df=1, max_df=0.95)\n",
    "doc_term_matrix = vectorizer.fit_transform(\n",
    "     (doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True)\n",
    "      for doc in corpus))\n",
    "id2term=vectorizer.id_to_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_content = textacy.vsm.get_information_content(doc_term_matrix)\n",
    "doc_freq = textacy.vsm.get_doc_freqs(doc_term_matrix)\n",
    "term_freq = textacy.vsm.get_term_freqs(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essay\n",
      "hello stupid\n",
      "fly cool\n"
     ]
    }
   ],
   "source": [
    "docfreq_dict  = {}\n",
    "for i, val in id2term.items():\n",
    "    docfreq_dict[val] = doc_freq[i]\n",
    "\n",
    "keyterms_dict = {}\n",
    "for i in corpus:\n",
    "    print(i.text)\n",
    "    try:\n",
    "        keyterms_dict[i.text] = textacy.keyterms.key_terms_from_semantic_network(i)[0][1]\n",
    "    except IndexError:\n",
    "        keyterms_dict[i.text] = 0.0\n",
    "        \n",
    "\n",
    "keyterms_sgrank_dict = {}\n",
    "for i in corpus:\n",
    "    try:\n",
    "        keyterms_sgrank_dict[i.text] = textacy.keyterms.key_terms_from_semantic_network(i)[0][1]\n",
    "    except IndexError:\n",
    "        keyterms_sgrank_dict[i.text] = 0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[apple orange pear]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in textacy.extract.ngrams(textacy.doc.Doc(\"apple orange pear\", lang = 'en_core_web_md'), 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model={}\n",
    "model['orange']=[0.1,0.9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 14])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import skew\n",
    "np.array([[1,3],[4,5],[5,6]]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc(3 tokens; \"apple orange pear\")\n",
      "Doc(2 tokens; \"fun great\")\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg=textacy.Doc(\"orange pear\",lang='en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-d6ec322bc686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_core_web_md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0messay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m doc_term_matrix, id2term = textacy.vsm.doc_term_matrix(\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_terms_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamed_entities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_strings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/kaggle/lib/python3.6/site-packages/en_core_web_md/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/kaggle/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, **overrides)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath2str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/kaggle/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, **overrides)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/kaggle/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(self, path, disable)\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'vocab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0mexclude\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/kaggle/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/kaggle/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m'meta.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             ('vocab', lambda p: (\n\u001b[0;32m--> 641\u001b[0;31m                 self.vocab.from_disk(p) and _fix_pretrained_vectors_name(self))),\n\u001b[0m\u001b[1;32m    642\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         ))\n",
      "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.from_disk\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mvectors.pyx\u001b[0m in \u001b[0;36mspacy.vectors.Vectors.from_disk\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/kaggle/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import en_core_web_md\n",
    "corpus = textacy.Corpus(en_core_web_md.load(), texts = essay)\n",
    "\n",
    "doc_term_matrix, id2term = textacy.vsm.doc_term_matrix(\n",
    "    (doc.to_terms_list(ngrams=3, named_entities=True, as_strings=True) for doc in corpus),\n",
    "     weighting='tfidf', normalize=True, smooth_idf=True, min_df=2, max_df=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path='data/df.p'\n",
    "if os.path.exists(df_path):\n",
    "    df=pd.read_pickle('data/df.p')\n",
    "else:\n",
    "    df.to_pickle('data/df.p')\n",
    "    \n",
    "txt_path = 'data/txt.npy'\n",
    "if os.path.exists(txt_path):\n",
    "    X_txt = pickle.load(open(txt_path, 'rb'))\n",
    "else:\n",
    "    pickle.dump(X_txt, open(txt_path, 'wb'))\n",
    "\n",
    "cat_path = 'data/cat.npy'\n",
    "if os.path.exists(cat_path):\n",
    "    X_cat = pickle.load(open(cat_path, 'rb'))\n",
    "else:\n",
    "    pickle.dump(X_cat, open(cat_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "numFeatures=[df.columns[i] for i,j in enumerate(df.dtypes) if j == 'float64' and not (df.columns[i]=='project_is_approved') ]\n",
    "X = sparse.hstack((X_txt, X_cat, StandardScaler().fit_transform(df[numFeatures].fillna(0)))).tocsr()\n",
    "\n",
    "Xtr = X[np.argwhere(-pd.isna(df.project_is_approved)).reshape(-1,)]\n",
    "Xts = X[np.argwhere(pd.isna(df.project_is_approved)).reshape(-1,)]\n",
    "Ttr_tar = df[-pd.isna(df.project_is_approved)]['project_is_approved'].values\n",
    "Tts = df[-pd.isna(df.project_is_approved)][['id','project_is_approved']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 154768 samples, validate on 27312 samples\n",
      "Epoch 1/1\n",
      " - 25s - loss: 0.4236 - binary_accuracy: 0.8376 - val_loss: 0.3662 - val_binary_accuracy: 0.8478\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36617, saving model to NN.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "from keras.layers import Input, Dense, Flatten, concatenate, Dropout, Embedding, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "n_es1, n_es2, n_prs, n_rd, n_pt = 3000, 8000, 2000, 3000, 1000\n",
    "def breakInput(X1):\n",
    "    X2 = []\n",
    "    i = 0\n",
    "    for n in [n_es1, n_es2, n_prs, n_rd, n_pt, X_cat.shape[1], len(numFeatures)]:\n",
    "        X2.append(X1[:,i:i+n])\n",
    "        i += n\n",
    "        \n",
    "    return X2\n",
    "\n",
    "def getModel(HLs, Drop=0.25, OP=optimizers.Adam()):\n",
    "    temp = []\n",
    "    inputs_txt = []\n",
    "    for n in [n_es1, n_es2, n_prs, n_rd, n_pt]:\n",
    "        input_txt = Input((n, ))\n",
    "        X_feat = Dropout(Drop)(input_txt)\n",
    "        X_feat = Dense(int(n/100), activation=\"linear\")(X_feat)\n",
    "        X_feat = Dropout(Drop)(X_feat)\n",
    "        temp.append(X_feat)\n",
    "        inputs_txt.append(input_txt)\n",
    "\n",
    "    x_1 = concatenate(temp)\n",
    "#     x_1 = Dense(20, activation=\"relu\")(x_1)\n",
    "    x_1 = Dense(50, activation=\"relu\")(x_1)\n",
    "    x_1 = Dropout(Drop)(x_1)\n",
    "\n",
    "    input_cat = Input((X_cat.shape[1], ))\n",
    "    x_2 = Embedding(2, 10, input_length=X_cat.shape[1])(input_cat)\n",
    "#     x_2 = SpatialDropout1D(Drop)(x_2)\n",
    "    x_2 = Flatten()(x_2)\n",
    "\n",
    "    input_num = Input((len(numFeatures), ))\n",
    "    x_3 = Dropout(Drop)(input_num)\n",
    "    \n",
    "    x = concatenate([x_1, x_2, x_3])\n",
    "\n",
    "    for HL in HLs:\n",
    "        x = Dense(HL, activation=\"relu\")(x)\n",
    "        x = Dropout(Drop)(x)\n",
    "\n",
    "    output = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs_txt+[input_cat, input_num], outputs=output)\n",
    "    model.compile(\n",
    "            optimizer=OP,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "def trainNN(X_train, X_val, Tar_train, Tar_val, HL=[50], Drop=0.5, OP=optimizers.Adam()):\n",
    "    file_path='NN.h5'\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=6)\n",
    "    lr_reduced = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                   factor=0.5,\n",
    "                                   patience=2,\n",
    "                                   verbose=1,\n",
    "                                   epsilon=3e-4,\n",
    "                                   mode='min')\n",
    "\n",
    "    model = getModel(HL, Drop, OP)\n",
    "    model.fit(breakInput(X_train), Tar_train, validation_data=(breakInput(X_val), Tar_val),\n",
    "                        verbose=2, epochs=1, batch_size=1000, callbacks=[early, lr_reduced, checkpoint])\n",
    "    model.load_weights(file_path)\n",
    "    return model\n",
    "\n",
    "nCV = 1 # should be ideally larger\n",
    "for i in range(21, 22):\n",
    "    X_train, X_val, Tar_train, Tar_val = train_test_split(Xtr, Ttr_tar, test_size=0.15, random_state=i, stratify=Ttr_tar)\n",
    "    model = trainNN(X_train, X_val, Tar_train, Tar_val, HL=[50], Drop=0.5, OP=optimizers.Adam())\n",
    "    Yvl3 = model.predict(breakInput(X_val)).squeeze()\n",
    "    Yts3 = model.predict(breakInput(Xts)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 154768 samples, validate on 27312 samples\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.7026 - binary_accuracy: 0.6654 - val_loss: 0.4124 - val_binary_accuracy: 0.8477\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.41241, saving model to NN.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "from keras.layers import Input, Dense, Flatten, concatenate, Dropout, Embedding, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def getModel(HLs, Drop=0.25, OP=optimizers.Adam()):\n",
    "    input_num = Input((len(numFeatures), ))\n",
    "    x_3 = Dropout(Drop)(input_num)\n",
    "    \n",
    "    x = x_3\n",
    "\n",
    "    for HL in HLs:\n",
    "        x = Dense(HL, activation=\"relu\")(x)\n",
    "        x = Dropout(Drop)(x)\n",
    "\n",
    "    output = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=input_num, outputs=output)\n",
    "    model.compile(\n",
    "            optimizer=OP,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "def trainNN(X_train, X_val, Tar_train, Tar_val, HL=[50], Drop=0.5, OP=optimizers.Adam()):\n",
    "    file_path='NN.h5'\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=6)\n",
    "    lr_reduced = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                   factor=0.5,\n",
    "                                   patience=2,\n",
    "                                   verbose=1,\n",
    "                                   epsilon=3e-4,\n",
    "                                   mode='min')\n",
    "\n",
    "    model = getModel(HL, Drop, OP)\n",
    "    model.fit(breakInput(X_train)[6], Tar_train, validation_data=(breakInput(X_val)[6], Tar_val),\n",
    "                        verbose=2, epochs=1, batch_size=1000, callbacks=[early, lr_reduced, checkpoint])\n",
    "    model.load_weights(file_path)\n",
    "    return model\n",
    "\n",
    "nCV = 1 # should be ideally larger\n",
    "for i in range(21, 22):\n",
    "    X_train, X_val, Tar_train, Tar_val = train_test_split(Xtr, Ttr_tar, test_size=0.15, random_state=i, stratify=Ttr_tar)\n",
    "    model = trainNN(X_train, X_val, Tar_train, Tar_val, HL=[50], Drop=0.5, OP=optimizers.Adam())\n",
    "    Yvl3 = model.predict(breakInput(X_val)[6]).squeeze()\n",
    "    Yts3 = model.predict(breakInput(Xts)[6]).squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages/sklearn/metrics/classification.py:1694: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages/sklearn/metrics/classification.py:1694: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg=model.predict(breakInput(X_train)[6]).squeeze()\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(Tar_train,gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8476859988283538"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg=model.predict(breakInput(X_val)[6]).squeeze()\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Tar_val,np.round(gg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] InputLayer  binary_classification/n_es1_input_layer: (?, 3000)\n",
      "[TL] DropoutLayer n_es1_drop1: keep:1.000000 is_fix:False\n",
      "[TL] DenseLayer  n_es1_dense: 30 identity\n",
      "[TL] DropoutLayer n_es1drop2: keep:1.000000 is_fix:False\n",
      "[TL] InputLayer  binary_classification/n_es2_input_layer: (?, 8000)\n",
      "[TL] DropoutLayer n_es2_drop1: keep:1.000000 is_fix:False\n",
      "[TL] DenseLayer  n_es2_dense: 80 identity\n",
      "[TL] DropoutLayer n_es2drop2: keep:1.000000 is_fix:False\n",
      "[TL] InputLayer  binary_classification/n_prs_input_layer: (?, 2000)\n",
      "[TL] DropoutLayer n_prs_drop1: keep:1.000000 is_fix:False\n",
      "[TL] DenseLayer  n_prs_dense: 20 identity\n",
      "[TL] DropoutLayer n_prsdrop2: keep:1.000000 is_fix:False\n",
      "[TL] InputLayer  binary_classification/n_rd_input_layer: (?, 3000)\n",
      "[TL] DropoutLayer n_rd_drop1: keep:1.000000 is_fix:False\n",
      "[TL] DenseLayer  n_rd_dense: 30 identity\n",
      "[TL] DropoutLayer n_rddrop2: keep:1.000000 is_fix:False\n",
      "[TL] InputLayer  binary_classification/n_pt_input_layer: (?, 1000)\n",
      "[TL] DropoutLayer n_pt_drop1: keep:1.000000 is_fix:False\n",
      "[TL] DenseLayer  n_pt_dense: 10 identity\n",
      "[TL] DropoutLayer n_ptdrop2: keep:1.000000 is_fix:False\n",
      "[TL] ConcatLayer binary_classification/input_concat_layer: axis: 1\n",
      "[TL] DenseLayer  n_pt: 50 identity\n",
      "[TL] DropoutLayer input_concat_layer_drop: keep:1.000000 is_fix:False\n",
      "[TL] EmbeddingInputlayer binary_classification/x2_embed: (2, 10)\n",
      "[TL] FlattenLayer binary_classification/flatten: 1000\n",
      "[TL] InputLayer  binary_classification/x3_layer: (?, 37)\n",
      "[TL] DenseLayer  x_dense_50: 50 relu\n",
      "[TL] DropoutLayer x_drop_50: keep:1.000000 is_fix:False\n",
      "[TL] DenseLayer  output_layer: 1 sigmoid\n",
      "[TL] InputLayer  binary_classification/n_es1_input_layer: (?, 3000)\n",
      "[TL] DropoutLayer n_es1_drop1: keep:1.000000 is_fix:False\n",
      "[TL]   skip DropoutLayer\n",
      "[TL] DenseLayer  n_es1_dense: 30 identity\n",
      "[TL] DropoutLayer n_es1drop2: keep:1.000000 is_fix:False\n",
      "[TL]   skip DropoutLayer\n",
      "[TL] InputLayer  binary_classification/n_es2_input_layer: (?, 8000)\n",
      "[TL] DropoutLayer n_es2_drop1: keep:1.000000 is_fix:False\n",
      "[TL]   skip DropoutLayer\n",
      "[TL] DenseLayer  n_es2_dense: 80 identity\n",
      "[TL] DropoutLayer n_es2drop2: keep:1.000000 is_fix:False\n",
      "[TL]   skip DropoutLayer\n",
      "[TL] InputLayer  binary_classification/n_prs_input_layer: (?, 2000)\n",
      "[TL] DropoutLayer n_prs_drop1: keep:1.000000 is_fix:False\n",
      "[TL]   skip DropoutLayer\n",
      "[TL] DenseLayer  n_prs_dense: 20 identity\n",
      "[TL] DropoutLayer n_prsdrop2: keep:1.000000 is_fix:False\n",
      "[TL]   skip DropoutLayer\n",
      "[TL] InputLayer  binary_classification/n_rd_input_layer: (?, 3000)\n",
      "[TL] DropoutLayer n_rd_drop1: keep:1.000000 is_fix:False\n",
      "[TL]   skip DropoutLayer\n",
      "[TL] DenseLayer  n_rd_dense: 30 identity\n",
      "[TL] DropoutLayer n_rddrop2: keep:1.000000 is_fix:False\n",
      "[TL]   skip DropoutLayer\n",
      "[TL] InputLayer  binary_classification/n_pt_input_layer: (?, 1000)\n",
      "[TL] DropoutLayer n_pt_drop1: keep:1.000000 is_fix:False\n",
      "[TL]   skip DropoutLayer\n",
      "[TL] DenseLayer  n_pt_dense: 10 identity\n",
      "[TL] DropoutLayer n_ptdrop2: keep:1.000000 is_fix:False\n",
      "[TL]   skip DropoutLayer\n",
      "[TL] ConcatLayer binary_classification/input_concat_layer: axis: 1\n",
      "[TL] DenseLayer  n_pt: 50 identity\n",
      "[TL] DropoutLayer input_concat_layer_drop: keep:1.000000 is_fix:False\n",
      "[TL]   skip DropoutLayer\n",
      "[TL] EmbeddingInputlayer binary_classification/x2_embed: (2, 10)\n",
      "[TL] FlattenLayer binary_classification/flatten: 1000\n",
      "[TL] InputLayer  binary_classification/x3_layer: (?, 37)\n",
      "[TL] DenseLayer  x_dense_50: 50 relu\n",
      "[TL] DropoutLayer x_drop_50: keep:1.000000 is_fix:False\n",
      "[TL]   skip DropoutLayer\n",
      "[TL] DenseLayer  output_layer: 1 sigmoid\n"
     ]
    }
   ],
   "source": [
    "def nn_batch_generator(X_data, y_data, batch_size,shuffle=False):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    if shuffle:\n",
    "        index = np.arange(np.shape(y_data)[0])\n",
    "        np.random.shuffle(index)\n",
    "        X =  X_data[index, :]\n",
    "        y =  y_data[index]\n",
    "    else:\n",
    "        index = np.arange(np.shape(y_data)[0])\n",
    "    while counter < number_of_batches:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].todense()\n",
    "        y_batch = y_data[index_batch]\n",
    "        counter += 1\n",
    "        yield np.array(X_batch),y_batch\n",
    "            \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "HLs=[50]\n",
    "drop=1.0\n",
    "learning_rate=0.001\n",
    "placholder={}\n",
    "input_net={}\n",
    "# define placeholder\n",
    "x2_placeholder= tf.placeholder(tf.int32, shape=[None, X_cat.shape[1]], name='x2')\n",
    "input_num=len(numFeatures)\n",
    "x3_placeholder = tf.placeholder(tf.float32, shape=[None, input_num], name='x3')\n",
    "for n,label in zip([n_es1, n_es2, n_prs, n_rd, n_pt],['n_es1', 'n_es2', 'n_prs', 'n_rd', 'n_pt']):\n",
    "    placholder[label] = tf.placeholder(tf.float32, shape=[None, n], name=label)    \n",
    "\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, ], name='target')\n",
    "\n",
    "def model(x, is_train=True, reuse=False):\n",
    "    placholder= x[0]\n",
    "    x2_placeholder= x[1]\n",
    "    x3_placeholder= x[2]\n",
    "    with tf.variable_scope(\"binary_classification\", reuse=reuse):\n",
    "        for n,label in zip([n_es1, n_es2, n_prs, n_rd, n_pt],['n_es1', 'n_es2', 'n_prs', 'n_rd', 'n_pt']):\n",
    "            input_net[label] = tl.layers.InputLayer(placholder[label], name=label +'_input_layer')\n",
    "            input_net[label] = tl.layers.DropoutLayer(input_net[label], keep=drop,is_train=is_train, name=label+'_drop1')\n",
    "            input_net[label] = tl.layers.DenseLayer(input_net[label], n_units=int(n/100), name=label+'_dense')\n",
    "            input_net[label] = tl.layers.DropoutLayer(input_net[label],is_train=is_train, keep=drop, name=label+'drop2')\n",
    "\n",
    "\n",
    "        x1=tl.layers.ConcatLayer([input_net[key] for key in input_net], 1, name ='input_concat_layer')\n",
    "        x1=tl.layers.DenseLayer(x1, n_units=50., name=label)\n",
    "        x1=tl.layers.DropoutLayer(x1, keep=drop,is_train=is_train, name='input_concat_layer_drop')\n",
    "\n",
    "        x2=tl.layers.EmbeddingInputlayer(inputs=x2_placeholder, vocabulary_size=2, embedding_size=10, name='x2_embed')\n",
    "        #x2=tl.layers.DropoutLayer(x2, keep=drop, name=label+'embed_drop')\n",
    "        x2 = tl.layers.FlattenLayer(x2)\n",
    "\n",
    "        x3 = tl.layers.InputLayer(x3_placeholder, name='x3_layer')\n",
    "        #x3 = tl.layers.DropoutLayer(x3, keep=drop,is_train=is_train, name='x3_embed_drop')\n",
    "\n",
    "        x=x3\n",
    "\n",
    "        for HL in HLs:\n",
    "            x = tl.layers.DenseLayer(x, n_units=50.,act=tf.nn.relu, name='x_dense_'+str(HL))\n",
    "            x = tl.layers.DropoutLayer(x, keep=drop,is_train=is_train, name='x_drop_'+str(HL))\n",
    "\n",
    "        x = tl.layers.DenseLayer(x, n_units=1,act=tf.nn.sigmoid,\n",
    "                                        name='output_layer')\n",
    "\n",
    "    return x\n",
    "\n",
    "net_train = model([placholder,x2_placeholder,x3_placeholder], is_train=True, reuse=False)\n",
    "net_test = model([placholder,x2_placeholder,x3_placeholder], is_train=False, reuse=True)\n",
    "\n",
    "\n",
    "y=net_train.outputs\n",
    "\n",
    "\n",
    "#cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.squeeze(y), labels=y_))\n",
    "cost = tl.cost.binary_cross_entropy(y, y_,name='xentropy')\n",
    "    \n",
    "\n",
    "y2 = net_test.outputs\n",
    "cost_test = tl.cost.binary_cross_entropy(y2, y_,name='xentropy2')\n",
    "correct_prediction = tf.equal(tf.round(y2), y_)\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "train_params = net_train.all_params\n",
    "\n",
    "\n",
    "train_op = tf.train.AdamOptimizer().minimize(cost,var_list=train_params)\n",
    "#train_op = tf.train.RMSPropOptimizer(learning_rate=0.001, momentum=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/envs/kaggle/lib/python3.6/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train loss: 644.199535\n",
      "   train acc: 0.723849\n",
      "   train loss: 636.924958\n",
      "   train acc: 0.741831\n",
      "   train loss: 629.824552\n",
      "   train acc: 0.758349\n",
      "   train loss: 622.951355\n",
      "   train acc: 0.774154\n",
      "   train loss: 616.253192\n",
      "   train acc: 0.789939\n",
      "   train loss: 609.734593\n",
      "   train acc: 0.803842\n",
      "   train loss: 603.410996\n",
      "   train acc: 0.815904\n",
      "   train loss: 597.289085\n",
      "   train acc: 0.825358\n",
      "   train loss: 591.314230\n",
      "   train acc: 0.832343\n",
      "   train loss: 585.500739\n",
      "   train acc: 0.837801\n",
      "   train loss: 579.839861\n",
      "   train acc: 0.841400\n",
      "   train loss: 574.335322\n",
      "   train acc: 0.843578\n",
      "   train loss: 568.985961\n",
      "   train acc: 0.845131\n",
      "   train loss: 563.793961\n",
      "   train acc: 0.846221\n",
      "   train loss: 558.728219\n",
      "   train acc: 0.846825\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tl.layers.initialize_global_variables(sess)\n",
    "\n",
    "n_epoch =50\n",
    "batch_size = 1000\n",
    "print_freq = 1000\n",
    "X_train, X_val, Tar_train, Tar_val = train_test_split(Xtr, Ttr_tar, test_size=0.15, random_state=i, stratify=Ttr_tar)\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    batch=nn_batch_generator(X_train, Tar_train,batch_size)\n",
    "    for x_t,y_t in batch:\n",
    "        x_t_es1, x_t_es2, x_t_prs, x_t_rd, x_t_pt,x_t_x2,x_t_x3 = breakInput(x_t)\n",
    "        y_t_batch = y_t\n",
    "        feed_dict={}\n",
    "        \n",
    "        feed_dict[placholder['n_es1']]=x_t_es1\n",
    "        feed_dict[placholder['n_es2']]=x_t_es2\n",
    "        feed_dict[placholder['n_prs']]=x_t_prs\n",
    "        feed_dict[placholder['n_rd']]=x_t_rd\n",
    "        feed_dict[placholder['n_pt']]=x_t_pt\n",
    "        feed_dict[x2_placeholder]=x_t_x2\n",
    "        feed_dict[x3_placeholder]=x_t_x3\n",
    "        feed_dict[y_]=y_t_batch\n",
    "        \n",
    "        feed_dict.update(net_train.all_drop)  # enable noise layers\n",
    "        sess.run(train_op, feed_dict=feed_dict)\n",
    "        #print(\"cost:{} accuray:{}\".format(c,a))\n",
    "        if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n",
    "            \n",
    "            train_loss, train_acc, n_batch = 0, 0, 0\n",
    "            batcha=nn_batch_generator(X_train, Tar_train,batch_size)\n",
    "            for x_t_a,y_t_a in batcha:\n",
    "                x_t_es1_a, x_t_es2_a, x_t_prs_a, x_t_rd_a, x_t_pt_a,x_t_x2_a,x_t_x3_a = breakInput(x_t_a)\n",
    "                y_t_batch_a = y_t_a\n",
    "                feed_dict_a={}\n",
    "                dp_dict = tl.utils.dict_to_one(net_train.all_drop)  # disable noise layers\n",
    "                \n",
    "                feed_dict_a[placholder['n_es1']]=x_t_es1_a\n",
    "                feed_dict_a[placholder['n_es2']]=x_t_es2_a\n",
    "                feed_dict_a[placholder['n_prs']]=x_t_prs_a\n",
    "                feed_dict_a[placholder['n_rd']]=x_t_rd_a\n",
    "                feed_dict_a[placholder['n_pt']]=x_t_pt_a\n",
    "                feed_dict_a[x2_placeholder]=x_t_x2_a\n",
    "                feed_dict_a[x3_placeholder]=x_t_x3_a\n",
    "                feed_dict_a[y_]=y_t_batch_a\n",
    "                feed_dict_a.update(dp_dict)\n",
    "\n",
    "                err, ac = sess.run([cost_test, acc], feed_dict=feed_dict_a)\n",
    "                train_loss += err\n",
    "                train_acc += ac\n",
    "                n_batch += 1\n",
    "            print(\"   train loss: %f\" % (train_loss / n_batch))\n",
    "            print(\"   train acc: %f\" % (train_acc / n_batch))\n",
    "            \n",
    "            val_loss, val_acc, val_n_batch = 0, 0, 0\n",
    "            batch_val=nn_batch_generator(X_val, Tar_train,batch_size)\n",
    "            for x_t_a_val,y_t_a_val in batch_val:                \n",
    "                x_val_t_es1, x_val_t_es2, x_val_t_prs, x_val_t_rd, x_val_t_pt,x_val_t_x2,x_val_t_x3 = breakInput()\n",
    "                y_val_t_batch = Tar_val\n",
    "                feed_dict_b={}\n",
    "                dp_dict = tl.utils.dict_to_one(net_train.all_drop)  # disable noise layers\n",
    "\n",
    "                feed_dict_b[placholder['n_es1']]=x_val_t_es1\n",
    "                feed_dict_b[placholder['n_es2']]=x_val_t_es2\n",
    "                feed_dict_b[placholder['n_prs']]=x_val_t_prs\n",
    "                feed_dict_b[placholder['n_rd']]=x_val_t_rd\n",
    "                feed_dict_b[placholder['n_pt']]=x_val_t_pt\n",
    "                feed_dict_b[x2_placeholder]=x_val_t_x2\n",
    "                feed_dict_b[x3_placeholder]=x_val_t_x3\n",
    "                feed_dict_b[y_]=y_val_t_batch\n",
    "                feed_dict_b.update(dp_dict)\n",
    "\n",
    "                val_cost,val_accuracy = sess.run([cost_test, acc], feed_dict=feed_dict_b)\n",
    "                val_loss += val_cost\n",
    "                val_acc += val_accuracy\n",
    "                val_n_batch += 1\n",
    "            print(\"validation train loss: %f\" % (val_loss / val_n_batch))\n",
    "            print(\"validation   train acc: %f\" % (val_acc / val_n_batch))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
